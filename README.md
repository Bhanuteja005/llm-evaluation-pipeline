# LLM Evaluation Pipeline

A production-grade evaluation pipeline for assessing LLM responses across three key dimensions:
1. **Relevance & Completeness** - Using semantic similarity with context
2. **Factual Accuracy** - Detecting hallucinations through claim verification
3. **Latency & Cost** - Performance and efficiency metrics

## Features

- ğŸ¯ **Comprehensive Evaluation**: Multi-dimensional scoring with detailed explanations
- ğŸ” **Semantic Search**: FAISS-based vector similarity for relevance checking
- ğŸ§  **Claim Extraction**: Automated factual claim detection and verification
- ğŸ“Š **SQLite Persistence**: Store and query evaluation results
- ğŸ¨ **Rich CLI**: Beautiful terminal output with tables and panels
- ğŸ³ **Docker Support**: Containerized deployment ready
- ğŸ§ª **Well-Tested**: Comprehensive test suite included
- ğŸ”Œ **Pluggable LLMs**: Mock or real LLM providers (OpenAI)

## Architecture

```
Input JSONs â†’ Ingest â†’ Prompt Builder â†’ LLM Client â†’ Evaluators â†’ Scorer â†’ Persistence
                                                          â†“
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚Relevance â”‚
                                                    â”‚Factual   â”‚
                                                    â”‚Latency   â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## Quick Start

### Usage

```bash


# Run evaluation
python -m src.cli evaluate \
  -c samples/sample-chat-conversation-01.json \
  -x samples/sample_context_vectors-01.json \
  -o results.json
```

### View Statistics

```bash
# Overall statistics
python -m src.cli stats

# Statistics for specific chat
python -m src.cli stats --chat-id 78128
```

## Input Format

### Conversation JSON

```json
{
  "chat_id": 78128,
  "user_id": 77096,
  "conversation_turns": [
    {
      "turn": 1,
      "sender_id": 1,
      "role": "AI/Chatbot",
      "message": "How can I help?",
      "created_at": "2025-01-01T10:00:00.000000Z"
    },
    {
      "turn": 2,
      "sender_id": 77096,
      "role": "User",
      "message": "What is the cost of IVF?",
      "created_at": "2025-01-01T10:01:00.000000Z"
    }
  ]
}
```

### Context Vectors JSON

```json
{
  "status": "success",
  "status_code": 200,
  "message": "Success",
  "data": {
    "vector_data": [
      {
        "id": 1,
        "source_url": "https://example.com/article",
        "text": "IVF costs approximately Rs 3,00,000...",
        "tokens": 50,
        "created_at": "2024-01-01T00:00:00.000Z"
      }
    ]
  }
}
```

## Output Format

```json
{
  "metadata": {
    "chat_id": 78128,
    "turn": 2,
    "timestamp": "2025-12-12T10:00:00.000000Z",
    "provider": "mock"
  },
  "metrics": {
    "relevance": {
      "relevance_score": 0.856,
      "completeness_score": 0.724,
      "top_k_context_ids": [1, 2, 3]
    },
    "factual_accuracy": {
      "hallucination_rate": 0.125,
      "verified_claims": 7,
      "total_claims": 8
    },
    "latency_cost": {
      "latency_ms": 245.3,
      "estimated_cost_usd": 0.000543
    }
  },
  "aggregate": {
    "overall_quality_score": 0.782,
    "passed_thresholds": true
  }
}
```

## Project Structure

```
llm-eval-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py              # CLI entrypoint
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ models.py           # Pydantic models
â”‚   â”œâ”€â”€ ingest.py           # Input loading
â”‚   â”œâ”€â”€ llm_client.py       # LLM wrappers
â”‚   â”œâ”€â”€ prompt_builder.py   # Prompt construction
â”‚   â”œâ”€â”€ extractors.py       # Claim extraction
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ relevance.py    # Relevance evaluation
â”‚   â”‚   â”œâ”€â”€ factual.py      # Factual checking
â”‚   â”‚   â””â”€â”€ latency_cost.py # Performance metrics
â”‚   â”œâ”€â”€ scoring.py          # Score aggregation
â”‚   â”œâ”€â”€ persistence.py      # Database storage
â”‚   â””â”€â”€ utils.py            # Utilities
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingest.py
â”‚   â”œâ”€â”€ test_relevance.py
â”‚   â”œâ”€â”€ test_factual.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ samples/                # Sample input files
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```
